{
 "cells": [
  {
   "cell_type": "code",
   "id": "b985c3ee58726dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:51:14.979453Z",
     "start_time": "2025-05-14T09:51:13.746441Z"
    }
   },
   "source": [
    "# GOT THIS SCRIPT FROM https://www.geeksforgeeks.org/text-classification-using-cnn/\n",
    "\n",
    "\n",
    "# importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Setting up the parameters\n",
    "maximum_features = 5000  # Maximum number of words to consider as features\n",
    "maximum_length = 100  # Maximum length of input sequences\n",
    "word_embedding_dims = 50  # Dimension of word embeddings\n",
    "no_of_filters = 250  # Number of filters in the convolutional layer\n",
    "kernel_size = 3  # Size of the convolutional filters\n",
    "hidden_dims = 250  # Number of neurons in the hidden layer\n",
    "batch_size = 32  # Batch size for training\n",
    "epochs = 2  # Number of training epochs\n",
    "threshold = 0.5  # Threshold for binary classification\n",
    "\n",
    "df = pd.read_csv('data/processed_data.csv')\n",
    "data = df['abstract'] + df['author_names']\n",
    "labels = df['cited_paper_id'].notnull().astype(int)  # Binary target: 1 for citing, 0 for non-citing\n",
    "\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The development of an automated system for the...\n",
       "1    This paper proposes a novel hybrid forward alg...\n",
       "2    Modern CCD cameras are usually capable of a sp...\n",
       "3    This paper deals with the problem of fuzzy non...\n",
       "4    A number of neural networks can be formulated ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:55:37.742086Z",
     "start_time": "2025-05-14T09:51:14.999228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "data['abstract'] = data['abstract'].fillna('').apply(remove_stopwords)\n",
    "\n",
    "\n",
    "#apply stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "data['abstract'] = data['abstract'].fillna('').apply(stemming)"
   ],
   "id": "f3e8ddd0cc27812",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'abstract'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m     10\u001B[39m     tokens = [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word.lower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words]\n\u001B[32m     11\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m.join(tokens)\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m data[\u001B[33m'\u001B[39m\u001B[33mabstract\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mabstract\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m.fillna(\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m).apply(remove_stopwords)\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m#apply stemming\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnltk\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mstem\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PorterStemmer\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:1121\u001B[39m, in \u001B[36mSeries.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1118\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[key]\n\u001B[32m   1120\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[32m-> \u001B[39m\u001B[32m1121\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1123\u001B[39m \u001B[38;5;66;03m# Convert generator to list before going through hashable part\u001B[39;00m\n\u001B[32m   1124\u001B[39m \u001B[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001B[39;00m\n\u001B[32m   1125\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:1237\u001B[39m, in \u001B[36mSeries._get_value\u001B[39m\u001B[34m(self, label, takeable)\u001B[39m\n\u001B[32m   1234\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[label]\n\u001B[32m   1236\u001B[39m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1237\u001B[39m loc = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1239\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[32m   1240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[loc]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\range.py:417\u001B[39m, in \u001B[36mRangeIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    415\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[32m--> \u001B[39m\u001B[32m417\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n\u001B[32m    418\u001B[39m \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'abstract'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Loading the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=maximum_features)\n",
    "\n",
    "# Padding the sequences to ensure uniform length\n",
    "x_train = pad_sequences(x_train, maxlen=maximum_length)\n",
    "x_test = pad_sequences(x_test, maxlen=maximum_length)"
   ],
   "id": "4788b247ca7829ed"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T09:55:37.745287800Z",
     "start_time": "2025-05-14T09:41:26.164592Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001B[1m17464789/17464789\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 0us/step\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 7ms/step - accuracy: 0.6978 - loss: 0.5284 - val_accuracy: 0.8509 - val_loss: 0.3339\n",
      "Epoch 2/2\n",
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 7ms/step - accuracy: 0.8992 - loss: 0.2505 - val_accuracy: 0.8565 - val_loss: 0.3296\n",
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step\n",
      "Accuracy: 0.85652\n",
      "Precision: 0.8357062146892655\n",
      "Recall: 0.88752\n",
      "F1-score: 0.8608341416100873\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the embedding layer to convert input sequences to dense vectors\n",
    "model.add(Embedding(maximum_features, word_embedding_dims,\n",
    "                    input_length=maximum_length))\n",
    "\n",
    "# Adding the 1D convolutional layer with ReLU activation\n",
    "model.add(Conv1D(no_of_filters, kernel_size, padding='valid',\n",
    "                 activation='relu', strides=1))\n",
    "\n",
    "# Adding the global max pooling layer to reduce dimensionality\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Adding the dense hidden layer with ReLU activation\n",
    "model.add(Dense(hidden_dims, activation='relu'))\n",
    "\n",
    "# Adding the output layer with sigmoid activation for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the model with binary cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Predicting the probabilities for test data\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Converting the probabilities to binary classes based on threshold\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# Calculating the evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Printing the evaluation metrics\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-score:', f1)"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
