{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-17T12:51:34.146244Z",
     "start_time": "2025-05-17T12:51:19.396002Z"
    }
   },
   "source": [
    "# Install SHAP if not already installed\n",
    "# pip install shap\n",
    "\n",
    "import shap\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset\n",
    "data = pd.read_csv('../data/training/positive/positive_data.csv')\n",
    "data['label'] = 1\n",
    "\n",
    "X = data.drop(columns=['label'])\n",
    "y = data['label']\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize SHAP values for a single prediction\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0], X_test.iloc[0])\n",
    "\n",
    "# Summary plot for feature importance\n",
    "shap.summary_plot(shap_values[1], X_test)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'We propose a method for learning semantic categories of words with minimal supervision from web search query logs. Our method is based on the Espresso algorithm (Pantel and Pennacchiotti, 2006) for extracting binary lexical relations, but makes important modifications to handle query log data for the task of acquiring semantic categories. We present experimental results comparing our method with two state-ofthe-art minimally supervised lexical knowledge extraction systems using Japanese query log data, and show that our method achieves higher precision than the previously proposed methods. We also show that the proposed method offers an additional advantage for knowledge acquisition in an Asian language for which word segmentation is an issue, as the method utilizes no prior knowledge of word segmentation, and is able to harvest new terms with correct word segmentation.'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_8708\\4010910799.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     15\u001B[39m \n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Train a model\u001B[39;00m\n\u001B[32m     17\u001B[39m model = RandomForestClassifier(random_state=\u001B[32m42\u001B[39m)\n\u001B[32m     18\u001B[39m X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u001B[32m0.2\u001B[39m, random_state=\u001B[32m42\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m model.fit(X_train, y_train)\n\u001B[32m     20\u001B[39m \n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# Initialize SHAP explainer\u001B[39;00m\n\u001B[32m     22\u001B[39m explainer = shap.TreeExplainer(model)\n",
      "\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1385\u001B[39m                 skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m                     prefer_skip_nested_validation \u001B[38;5;28;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m                 )\n\u001B[32m   1388\u001B[39m             ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, X, y, sample_weight)\u001B[39m\n\u001B[32m    356\u001B[39m         \u001B[38;5;66;03m# Validate or convert input data\u001B[39;00m\n\u001B[32m    357\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m issparse(y):\n\u001B[32m    358\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m ValueError(\u001B[33m\"sparse multilabel-indicator for y is not supported.\"\u001B[39m)\n\u001B[32m    359\u001B[39m \n\u001B[32m--> \u001B[39m\u001B[32m360\u001B[39m         X, y = validate_data(\n\u001B[32m    361\u001B[39m             self,\n\u001B[32m    362\u001B[39m             X,\n\u001B[32m    363\u001B[39m             y,\n",
      "\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2957\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"estimator\"\u001B[39m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01min\u001B[39;00m check_y_params:\n\u001B[32m   2958\u001B[39m                 check_y_params = {**default_check_params, **check_y_params}\n\u001B[32m   2959\u001B[39m             y = check_array(y, input_name=\u001B[33m\"y\"\u001B[39m, **check_y_params)\n\u001B[32m   2960\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2961\u001B[39m             X, y = check_X_y(X, y, **check_params)\n\u001B[32m   2962\u001B[39m         out = X, y\n\u001B[32m   2963\u001B[39m \n\u001B[32m   2964\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m no_val_X \u001B[38;5;28;01mand\u001B[39;00m check_params.get(\u001B[33m\"ensure_2d\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[39m\n\u001B[32m   1366\u001B[39m         )\n\u001B[32m   1367\u001B[39m \n\u001B[32m   1368\u001B[39m     ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001B[32m   1369\u001B[39m \n\u001B[32m-> \u001B[39m\u001B[32m1370\u001B[39m     X = check_array(\n\u001B[32m   1371\u001B[39m         X,\n\u001B[32m   1372\u001B[39m         accept_sparse=accept_sparse,\n\u001B[32m   1373\u001B[39m         accept_large_sparse=accept_large_sparse,\n",
      "\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1052\u001B[39m                         )\n\u001B[32m   1053\u001B[39m                     array = xp.astype(array, dtype, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1054\u001B[39m                 \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1055\u001B[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001B[32m-> \u001B[39m\u001B[32m1056\u001B[39m             \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[32m   1057\u001B[39m                 raise ValueError(\n\u001B[32m   1058\u001B[39m                     \u001B[33m\"Complex data not supported\\n{}\\n\"\u001B[39m.format(array)\n\u001B[32m   1059\u001B[39m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m complex_warning\n",
      "\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_array_api.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(array, dtype, order, copy, xp, device)\u001B[39m\n\u001B[32m    835\u001B[39m         \u001B[38;5;66;03m# Use NumPy API to support order\u001B[39;00m\n\u001B[32m    836\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    837\u001B[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001B[32m    838\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m839\u001B[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001B[32m    840\u001B[39m \n\u001B[32m    841\u001B[39m         \u001B[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001B[39;00m\n\u001B[32m    842\u001B[39m         \u001B[38;5;66;03m# container that is consistent with the input's namespace.\u001B[39;00m\n",
      "\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, dtype, copy)\u001B[39m\n\u001B[32m   2149\u001B[39m     def __array__(\n\u001B[32m   2150\u001B[39m         self, dtype: npt.DTypeLike | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m, copy: bool_t | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2151\u001B[39m     ) -> np.ndarray:\n\u001B[32m   2152\u001B[39m         values = self._values\n\u001B[32m-> \u001B[39m\u001B[32m2153\u001B[39m         arr = np.asarray(values, dtype=dtype)\n\u001B[32m   2154\u001B[39m         if (\n\u001B[32m   2155\u001B[39m             astype_is_view(values.dtype, arr.dtype)\n\u001B[32m   2156\u001B[39m             \u001B[38;5;28;01mand\u001B[39;00m using_copy_on_write()\n",
      "\u001B[31mValueError\u001B[39m: could not convert string to float: 'We propose a method for learning semantic categories of words with minimal supervision from web search query logs. Our method is based on the Espresso algorithm (Pantel and Pennacchiotti, 2006) for extracting binary lexical relations, but makes important modifications to handle query log data for the task of acquiring semantic categories. We present experimental results comparing our method with two state-ofthe-art minimally supervised lexical knowledge extraction systems using Japanese query log data, and show that our method achieves higher precision than the previously proposed methods. We also show that the proposed method offers an additional advantage for knowledge acquisition in an Asian language for which word segmentation is an issue, as the method utilizes no prior knowledge of word segmentation, and is able to harvest new terms with correct word segmentation.'"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
