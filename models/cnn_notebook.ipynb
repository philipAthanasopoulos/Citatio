{
 "cells": [
  {
   "cell_type": "code",
   "id": "87ee0596175c4b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:35:51.888624Z",
     "start_time": "2025-05-14T09:32:51.237056Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords and apply stemming\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/processed_data.csv')\n",
    "\n",
    "\n",
    "# Assume 'abstract' is the text column and 'cited_paper_id' is the target column\n",
    "df['abstract'] = df['abstract'].fillna('').apply(preprocess_text)  # Preprocess the text\n",
    "texts = df['abstract'] + df['author_names'] # Preprocessed abstracts\n",
    "labels = df['cited_paper_id'].notnull().astype(int)  # Binary target: 1 for citing, 0 for non-citing\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=10000)  # Use the top 10,000 words\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = 200  # Maximum sequence length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:38:35.071095Z",
     "start_time": "2025-05-14T09:35:56.492601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),  # Embedding layer\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),  # Convolutional layer\n",
    "    GlobalMaxPooling1D(),  # Global max pooling\n",
    "    Dense(64, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate classification report\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "915cba8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2770/2770\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 10ms/step - accuracy: 0.7246 - loss: 0.5677 - val_accuracy: 0.7272 - val_loss: 0.5416\n",
      "Epoch 2/5\n",
      "\u001B[1m2770/2770\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 14ms/step - accuracy: 0.7356 - loss: 0.5172 - val_accuracy: 0.7298 - val_loss: 0.5443\n",
      "Epoch 3/5\n",
      "\u001B[1m2770/2770\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 10ms/step - accuracy: 0.7760 - loss: 0.4503 - val_accuracy: 0.7253 - val_loss: 0.5737\n",
      "Epoch 4/5\n",
      "\u001B[1m2770/2770\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 10ms/step - accuracy: 0.8528 - loss: 0.3296 - val_accuracy: 0.6838 - val_loss: 0.7381\n",
      "Epoch 5/5\n",
      "\u001B[1m2770/2770\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 10ms/step - accuracy: 0.9245 - loss: 0.1954 - val_accuracy: 0.6690 - val_loss: 0.9238\n",
      "\u001B[1m866/866\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 3ms/step - accuracy: 0.6633 - loss: 0.9511\n",
      "Test Accuracy: 0.67\n",
      "\u001B[1m866/866\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.40      0.40      7559\n",
      "           1       0.77      0.76      0.77     20141\n",
      "\n",
      "    accuracy                           0.67     27700\n",
      "   macro avg       0.58      0.58      0.58     27700\n",
      "weighted avg       0.67      0.67      0.67     27700\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "c50951f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:39:01.863373Z",
     "start_time": "2025-05-14T09:38:46.110224Z"
    }
   },
   "source": [
    "# Preprocess the abstracts\n",
    "\n",
    "# Tokenize the abstracts\n",
    "abstracts_seq = tokenizer.texts_to_sequences(df['abstract'].iloc[:106692] + df['author_names'].iloc[:106692])\n",
    "\n",
    "# Pad the sequences\n",
    "abstracts_padded = pad_sequences(abstracts_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Predict using the model\n",
    "pred = model.predict(abstracts_padded)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3335/3335\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 3ms/step\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:39:04.674502Z",
     "start_time": "2025-05-14T09:39:04.539121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = pd.DataFrame()\n",
    "result['ID'] = df['paper_id'].iloc[:106692]\n",
    "result['Label'] = pred \n",
    "result.to_csv('data/predictions.csv', index=False)"
   ],
   "id": "83303253ca480873",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
